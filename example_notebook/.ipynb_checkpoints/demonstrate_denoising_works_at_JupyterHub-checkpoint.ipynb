{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False # developing only, remove later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# import glob\n",
    "\n",
    "from denoising import denoising\n",
    "from denoising.fit_correlation import extract_parameters_single, extract_parameters\n",
    "from denoising.fit_parameters import *\n",
    "import denoising.g2 as G2\n",
    "from denoising.file_handling import write_to_file, load_from_file, load_result_dict_from_file\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indicate locations of model-related files\n",
    "\n",
    "There is an option to select kernel of the model (kernel =1, 3 and 17 are avaiable). \n",
    "Smaller kernels are less sensitive to small variations, i.e return more smooth 2TCF, and takes less time to compute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = 1\n",
    "\n",
    "model_path = f'/nsls2/data/projects/ldrd_20_xpcs_ml/model_files/models/model_kernel_{kernel}.pt'\n",
    "rec_errors_training_path  = f'/nsls2/data/projects/ldrd_20_xpcs_ml/model_files/rec_errors/reconstruction_error_kernel_{kernel}.pt'\n",
    "latent_space_scaler_file = f'/nsls2/data/projects/ldrd_20_xpcs_ml/model_files/lat_space/lat_space_scaler_kernel_{kernel}.pt'\n",
    "distance_ecdf_file = f'/nsls2/data/projects/ldrd_20_xpcs_ml/model_files/lat_space/ecdf_kernel_{kernel}.pt'\n",
    "distance_median_file = f'/nsls2/data/projects/ldrd_20_xpcs_ml/model_files/lat_space/median_lat_distance_kernel_{kernel}.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting an example of 2TCF\n",
    "# Shape is (N_frames, N_frames, N_roi) or (N_roi, N_frames, N_frames). \n",
    "# After denosing the shape is (N_frames, N_frames, N_roi)\n",
    "\n",
    "temp = np.load('test_g2.npy') \n",
    "C_inf = np.nanmean(temp[:30, -30:])\n",
    "plt.figure(dpi = 130)\n",
    "\n",
    "\n",
    "z = np.empty((temp.shape[0],temp.shape[0], 4))\n",
    "z[:, :, 0] = temp\n",
    "z[:, :, 1] = temp\n",
    "z[:, :, 2] = temp\n",
    "z[:, :, 3] = temp\n",
    "\n",
    "plt.imshow(z[:, :, 0], origin = 'lower', vmin = 1, vmax = 1.2)\n",
    "sh = z.shape[0]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Denoising the 2TCF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the denoising model.  \n",
    "\n",
    "First, need to select a scorer - method for identifying the confidence (accuracy) score. There are three methods currently offered.   \n",
    "\n",
    "* `AutoCorrScoring` is based on *probability density function* of autocorrelation coefficient of the residual. The residual is calculated as `autocorr_coeff(np.mean((output-input), 1), lag = 1)`. It is a good indication of whether the denoised results systematically over-fit or under-fit the actual data. Can also be an indication of heterogeneities. This is a somewhat interpretative score. Its value are in range [0,1] with 0 being the worst confidence and 1 being the best confidence. Note, the value 1 is only reached at a single point and the values in range [0.3, 1] are relatively good.    \n",
    "\n",
    "\n",
    "* `LatentSpaceScoring` with `mode='probability'` is based on the distribution of the latent space representation of the training set. It is the *empirical cumulative distribution function* of distances between the center of the latent space and the point, corresponding to the model input. The examples from the training set are grouped around the center of the latent space. The further the point from the center, the more it is different from the training. set. The values of this scoring are in range [0,1] with 0 being the best and 1 being the worst. Note, this method does not allow to distinguish between two points that are both further away from the center than any of the training set example.\n",
    "\n",
    "\n",
    "* `LatentSpaceScoring` with `mode='relative_distance'` is based on the distribution of the latent space representation of the training set. It is equal to the ratio between two distances. Distance #1 is between the latent space representation of the input and the center of the latent space. Distance #2 is the median of such distance for the training set. The values can be from 0 to *infinity* with the larger values being the worse. \n",
    "  \n",
    "\n",
    "**For the `LatentSpaceScoring`, all coordinates are scaled to zero mean and unit variance based on the training set.\n",
    "\n",
    "***If we record the confidence score, we need to indicate what type of scoring it is.\n",
    "\n",
    "\n",
    "Second, initialize a G2Denoiser instance based on the model and the scorer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score based on the latent space are low for accurate predictions! \n",
    "# In oppose to scoring based on autocorrelation of the residuals\n",
    "\n",
    "\n",
    "scorer = denoising.AutoCorrScoring(model_path, rec_errors_training_path )\n",
    "\n",
    "# scorer = denoising.LatentSpaceScoring(autoencoder_filepath = model_path,\n",
    "#                                                 latent_space_scaler_file = latent_space_scaler_file,\n",
    "#                                                 distance_ecdf_file = distance_ecdf_file,\n",
    "#                                                 distance_median_file = distance_median_file,\n",
    "#                                                 mode = 'probability')\n",
    "\n",
    "# initiate denoising intanse\n",
    "denoiser = denoising.G2Denoiser( model_path , scorer ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the denoising model to the all 2TCFs\n",
    "# If a single ROI is passed - reshape\n",
    "\n",
    "res, conf_res = denoiser.remove_noise( z, # z in 2TCF\n",
    "            coarse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_start = 0\n",
    "fast_end = 200\n",
    "\n",
    "# calculate fast portion\n",
    "res_fast, conf_res_fast = denoiser.remove_noise(\n",
    "                          z[fast_start:fast_end, fast_start:fast_end, :] , coarse = True)\n",
    "# calculate all\n",
    "res, conf_res = denoiser.remove_noise( z[:, :],coarse = True)\n",
    "\n",
    "# replace fast part of the whole image with the fast denoised part\n",
    "res[fast_start:fast_end, fast_start:fast_end, :] = res_fast\n",
    "conf_res[fast_start:fast_end, fast_start:fast_end, :] = conf_res_fast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving denoised data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "filename = \"two_time_data.hdf5\"\n",
    "f1 = h5py.File(filename, \"w\")\n",
    "raw_dset = f1.create_dataset(\"raw\", z.shape, data=z)\n",
    "denoised_dset = f1.create_dataset(\"denoised\", res.shape, data=res)\n",
    "confidence_score_dset = f1.create_dataset(\"confidence_score\", conf_res.shape, data=conf_res)\n",
    "confidence_score_dset.attrs['score_type'] = 'correlation'\n",
    "f1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example of reading the saved data\n",
    "# f2 = h5py.File('two_time_data.hdf5', 'r')\n",
    "# raw = f2['raw'][:]\n",
    "# denoised_dset = f2['denoised'][:]\n",
    "# confidence_score_dset = f2['confidence_score']\n",
    "# confidence_score = confidence_score_dset[:]\n",
    "# print(confidence_score_dset.attrs['score_type'])\n",
    "# f2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualizing the results of the denoising\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "z1 = res[:, :, -1]\n",
    "confidence_score = conf_res[:, :, -1]\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1,3, dpi = 150, figsize = (10,3))\n",
    "\n",
    "ax[0].imshow(z[ :, :, -1], vmin = 1, vmax =1.2, origin = 'lower')\n",
    "divider0 = make_axes_locatable(ax[0])\n",
    "cax0 = divider0.append_axes(\"right\", size=\"3%\", pad=0.1)\n",
    "cax0.axis('off')\n",
    "\n",
    "ax[1].imshow(z1[:, :], vmin = 1, vmax =1.2, origin = 'lower')\n",
    "divider1 = make_axes_locatable(ax[1])\n",
    "cax1 = divider1.append_axes(\"right\", size=\"3%\", pad=0.1)\n",
    "cax1.axis('off')\n",
    "\n",
    "scoring_img = ax[2].imshow(confidence_score, vmin = 0, vmax =1, cmap = 'plasma', origin = 'lower')\n",
    "divider2 = make_axes_locatable(ax[2])\n",
    "cax2 = divider2.append_axes(\"right\", size=\"3%\", pad=0.1)\n",
    "cbar = fig.colorbar(scoring_img, cax=cax2, ax=ax[2], ticks=[0, 1])\n",
    "\n",
    "# preparing tick labels for scorring\n",
    "if isinstance(scorer, denoising.AutoCorrScoring):\n",
    "    colorbar_tick_labels = ['bad', 'good']\n",
    "else:\n",
    "    colorbar_tick_labels = ['good', 'bad']\n",
    "    \n",
    "cbar.ax.set_yticklabels(colorbar_tick_labels)\n",
    "\n",
    "ax[0].set_title('raw')\n",
    "ax[0].set_xlabel('lag frames')\n",
    "ax[0].set_ylabel('lag frames')\n",
    "ax[1].set_title('denoised')\n",
    "ax[1].set_xlabel('lag frames')\n",
    "ax[2].set_title('condifence scoring')\n",
    "# ax[2].set_title('difference')\n",
    "ax[2].set_xlabel('lag frames')\n",
    "# for j in range(3):\n",
    "#     ax[j].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting the dynamics parameters\n",
    "\n",
    "Only single stretched exponent fit is fully functioning now. Prior to fit, one needs to initialize an instance of `Stretched_Exponent_Parameters`, containing lower/upper bounds of the parameters and their starting values. All parameters except for parameters name ('beta', 'gamma', etc.) are optional and can be given default values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize fitting parameters if needed\n",
    "stretched_exp_params = Stretched_Exponent_Parameters(\n",
    "    [ factory_make_fit_parameter( 'beta'  , lower=0    , upper=1 ) ,\n",
    "      factory_make_fit_parameter( 'gamma' , lower=0    , upper=2 ) ,\n",
    "      factory_make_fit_parameter( 'ginf'  , lower=0.99 , upper=5.0) ,\n",
    "      factory_make_fit_parameter( 'alpha' , lower=0.2  , upper=3 ) ] \n",
    "    )\n",
    "\n",
    "### OR contract fit parameters from a previous log file (if avaiable)\n",
    "# previous_log_file = 'fit_parameter_log.json'\n",
    "# stretched_exp_params = make_stretched_exponent_parameter_list_from_file(previous_log_file)\n",
    "\n",
    "# check the construction\n",
    "print(stretched_exp_params.get_initial_values())\n",
    "print(stretched_exp_params.get_param_bounds())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are some of the theshold parameters used later for identifying the trust regions of the fitting results (contrast and baseline). The default values are defined by the code.\n",
    "\n",
    "trust_region_lower/upper_bound are based on the `half_time` (the time it takes for the correlation function to drop by half, calculated from the fit results: rate and compression constant). If you want to change this vales, it must be done *before* the function `extract_parameters` is called.\n",
    "\n",
    "`trust_region_lower_bound` indicates the smallest value of the `half_time` to count the results of the fitting for contrast as reliable. That is, if the hal-time is smaller than `trust_region_lower_bound` (measured in frames), then the the results value for the contrast is \"not trustable\".\n",
    "\n",
    "\n",
    "`trust_region_upper_bound_coef` indicates the upper bound (measured in `half_time`s) for the results for the contrast and baseline to be considered \"trustworthy\". That is `half_time` * `trust_region_upper_bound_coef` indicates the minimum number of frames should be contained in 1TCF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ###option to change the trust region cut-off by the time\n",
    "import denoising.fit_correlation\n",
    "denoising.fit_correlation.TRUST_REGION_LOWER_BOUND = 5 # minimum number of frames for contrast to drop by half\n",
    "denoising.fit_correlation.TRUST_REGION_UPPER_BOUND_COEF = 2 # minimum number of half-decay periods avaiable for the fit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<a id='the_destination'></a> **Fitting the 2TCF**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Extracting parameters. Fit for multiple ROIs\n",
    "\n",
    "# identify the files to store the fit parameters and \n",
    "save_log_filename = 'fit_parameter_log'\n",
    "save_results_m_filename = 'fit_results_m'\n",
    "save_results_raw_filename = 'fit_results_raw'\n",
    "\n",
    "# list the ROIs you want to analyze, indexing starts from 0 (for now). Need to list in the ascending order.\n",
    "roi_list = [1,2]\n",
    "\n",
    "# list time steps for corresponding ROI list. Time is in frames.\n",
    "time_steps = [2,5]\n",
    "\n",
    "# fit denoised data\n",
    "result_m = extract_parameters(res,\n",
    "                              save_log_filename=save_log_filename,\n",
    "                              save_results_filename=save_results_m_filename,\n",
    "                              stretched_exp_params_list = [stretched_exp_params]*len(roi_list),\n",
    "                              steps = time_steps,\n",
    "                              roi_list = roi_list) \n",
    "# # fit raw data\n",
    "result_raw = extract_parameters(z,\n",
    "                                save_log_filename=save_log_filename,\n",
    "                                save_results_filename=save_results_raw_filename,\n",
    "                                stretched_exp_params_list = [stretched_exp_params]*len(roi_list),\n",
    "                                steps = time_steps,\n",
    "                                roi_list = roi_list) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_m[2]['lag_times'] \n",
    "# Right now time is the right edge of the interval.\n",
    "# Should it be left? Or middle?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results of the fit\n",
    "\n",
    "fig, ax = plt.subplots(5,1 , dpi= 150, figsize = (4.5,9))\n",
    "params = ['beta', 'gamma', 'ginf', 'alpha']\n",
    "roi = 1\n",
    "\n",
    "for j in range(4):\n",
    "    p = params[j]    \n",
    "    ax[j].plot(result_raw[roi]['lag_times']['frames'],result_raw[roi]['parameters'][p], label = 'raw', alpha = 0.5)\n",
    "    ax[j].plot(result_m[roi]['lag_times']['frames'],result_m[roi]['parameters'][p], label = 'denoised',color = 'red', alpha = 0.5)\n",
    "#     ax[j].set_xlabel('Time')\n",
    "\n",
    "ax[1].set_ylim((0, 0.1))\n",
    "ax[0].set_ylabel(r'$\\beta$')\n",
    "ax[1].set_ylabel(r'$\\Gamma [frame^{-1}]$')\n",
    "ax[2].set_ylabel(r'$C_{\\infty}$')\n",
    "ax[3].set_ylabel(r'$\\alpha$')\n",
    "ax[4].set_ylabel(r'$R^{2}$')\n",
    "\n",
    "\n",
    "ax[4].plot(result_raw[roi]['lag_times']['frames'],result_raw[roi]['fit_quality']['r2_fit'], label = 'raw', alpha = 0.5)\n",
    "ax[4].plot(result_m[roi]['lag_times']['frames'],result_m[roi]['fit_quality']['r2_fit'], label = 'model', alpha = 0.9)\n",
    "ax[4].set_xlabel('Time [frames]')\n",
    "fig.align_ylabels()\n",
    "plt.legend(bbox_to_anchor=(1.05, 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the adjustment of the `trust_regions` for `beta` and `ginf`. Several options are shown: by R2 value, by correlation between parameters, by relative error, manual selection of 'bad' frames. All thresholds are given for illustration purpose and may be not optimal for each individual case.\n",
    "These parameters can be interactevely changed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjusting other thresholds #these may be ROI-specific\n",
    "r2_treshold = 0.85 \n",
    "corr_threshold = 0.85\n",
    "rel_err_threshold = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore the trust regions for individual ROIs\n",
    "roi = 1\n",
    "trust_beta_m  = result_m[roi]['trust_regions']['beta'].copy()\n",
    "trust_g_inf_m = result_m[roi]['trust_regions']['ginf'].copy()\n",
    "\n",
    "\n",
    "# # additional criteria for selecting trust regions\n",
    "\n",
    "# #R2\n",
    "r2_treshold = 0.85\n",
    "r2_m = result_m[roi]['fit_quality']['r2_fit']\n",
    "trust_beta_m[r2_m < r2_treshold] = False\n",
    "trust_g_inf_m[r2_m < r2_treshold] = False\n",
    "\n",
    "# # correlations\n",
    "corr_threshold = 0.85\n",
    "beta_g_inf_corr = result_m[roi]['correlations']['corr_beta_ginf']\n",
    "beta_gamma_corr = result_m[roi]['correlations']['corr_beta_gamma']\n",
    "trust_beta_m[beta_g_inf_corr > corr_threshold] = False\n",
    "trust_beta_m[beta_gamma_corr > corr_threshold] = False\n",
    "trust_g_inf_m[beta_g_inf_corr > corr_threshold] = False\n",
    "\n",
    "# # based on error of the parameter\n",
    "rel_err_threshold = 1\n",
    "beta_m = result_m[roi]['parameters']['beta']\n",
    "beta_err_m = result_m[roi]['parameters_errors']['beta']\n",
    "trust_beta_m[beta_err_m/beta_m > rel_err_threshold] = False\n",
    "\n",
    "# manual replacement\n",
    "trust_beta_m[250:275] = False\n",
    "\n",
    "\n",
    "#compare with the fit of raw data\n",
    "beta_raw = result_raw[roi]['parameters']['beta']\n",
    "\n",
    "plt.figure(dpi = 200)\n",
    "plt.plot(beta_raw, label = 'raw fit results')\n",
    "plt.plot(beta_m, label = 'model fit results')\n",
    "plt.fill_between(np.arange(beta_m.shape[0]), trust_beta_m, 0, alpha = 0.2, label = 'trust region')\n",
    "plt.plot(beta_g_inf_corr, label = r'Corr($\\beta, g_{\\infty})$')\n",
    "plt.ylabel(r'$\\beta$')\n",
    "plt.xlabel('time [frames]')\n",
    "plt.legend()\n",
    "plt.ylim(-0.01,1.01)\n",
    "plt.title(r'Trust regions for $\\beta$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update the results for the trust region if needed\n",
    "result_m[roi]['trust_regions']['beta'] = trust_beta_m\n",
    "result_m[roi]['trust_regions']['g_inf'] = trust_g_inf_m\n",
    "\n",
    "# save the final fit\n",
    "from denoising.file_handling import write_to_file\n",
    "write_to_file(result_m, 'final_fit_results_model')\n",
    "\n",
    "#can do the same for results_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, can use values from the `trust_regions` to establish new parameters bounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update the parameter bounds for fit parameters based on the range of trust regions (return to cell #7,\n",
    "# but do not initialize new stretched_exp_params instance)\n",
    "roi = 1\n",
    "beta_confident_values = result_m[roi]['parameters']['beta'][trust_beta_m.astype('bool')]\n",
    "beta_low = np.min(beta_confident_values)\n",
    "beta_high = np.max(beta_confident_values)\n",
    "\n",
    "\n",
    "ginf_confident_values = result_m[roi]['parameters']['ginf'][trust_g_inf_m.astype('bool')]\n",
    "ginf_low = np.min(ginf_confident_values)\n",
    "ginf_high = np.max(ginf_confident_values)\n",
    "\n",
    "\n",
    "# initialize fitting parameters if needed \n",
    "# TODO: should we make a function to update only certain parameters without typing everything again?\n",
    "\n",
    "stretched_exp_params = Stretched_Exponent_Parameters(\n",
    "    [ factory_make_fit_parameter( 'beta'  , lower=beta_low , upper=beta_high ) ,\n",
    "      factory_make_fit_parameter( 'gamma' , lower=0        , upper=2 ) ,\n",
    "      factory_make_fit_parameter( 'ginf'  , lower=ginf_low , upper=ginf_high ) ,\n",
    "      factory_make_fit_parameter( 'alpha' , lower=0.2      , upper=3 )\n",
    "       ] \n",
    "    )\n",
    "\n",
    "print(stretched_exp_params.get_initial_values())\n",
    "print(stretched_exp_params.get_param_bounds())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[Return to the fit to run with new parameters bounds](#the_destination)\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-analysis 2TCF reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from denoising.post_fit_analysis import *\n",
    "result_m = load_result_dict_from_file('final_fit_results_model.json')\n",
    "result_raw = load_result_dict_from_file('fit_results_raw.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reconstructing for a single ROI\n",
    "roi = 1\n",
    "a = reconstruct_2tcf_from_fit_parameters_single_roi(result_m, z.shape[0], roi)\n",
    "plt.imshow(a, vmin = 1, vmax = 1.15, origin = 'lower')\n",
    "plt.xlabel(r'$t_{1}$ [frames]')\n",
    "plt.ylabel(r'$t_{2}$ [frames]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reconstruct all 2CTF from the fitting results\n",
    "all_2tcf_rec = reconstruct_2tcf_from_fit_parameters(result_m, z.shape[0])\n",
    "roi = 2\n",
    "plt.imshow(all_2tcf_rec[2], vmin = 1, vmax = 1.15, origin = 'lower')\n",
    "plt.xlabel(r'$t_{1}$ [frames]')\n",
    "plt.ylabel(r'$t_{2}$ [frames]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at reconstruction at custom delays\n",
    "reconstructed, trust_beta = get_full_reconstructed_2tcf_single_roi(result_m, 350,200, 1)\n",
    "\n",
    "# plot the reconstruction\n",
    "plt.figure(dpi = 150)\n",
    "plt.title('Overlay of reconstructed 2TCF and beta_trust_region')\n",
    "plt.imshow(reconstructed, vmin = 1, vmax = 1.10)\n",
    "plt.imshow(-trust_beta, alpha = 0.2, cmap = 'spring', vmin = -1, vmax = 0)\n",
    "plt.ylabel('experiment time [frames]')\n",
    "plt.xlabel('lag time [frames]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting latent space coordinates\n",
    "\n",
    "This is for testing and developing purposes, not a part of analysis. Need more work to figure out what data to collect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from denoising.latent_space import LatentSpaceExtractor\n",
    "\n",
    "\n",
    "\n",
    "lat_space_extractor = LatentSpaceExtractor(autoencoder_filepath = model_path,\n",
    "                                          latent_space_scaler_file = latent_space_scaler_file,\n",
    "                                          distance_ecdf_file = distance_ecdf_file,\n",
    "                                          distance_median_file = distance_median_file)\n",
    "\n",
    "inpt = z[1::3, 1::3][:100, :100, 0]\n",
    "lat_space = lat_space_extractor.get_latent_space(G2.G2(inpt))\n",
    "distance = lat_space_extractor.calculate_distance_to_training_center(lat_space)\n",
    "lat_space_extractor.calc_prob_of_distance_too_big(distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
